{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements language model to model text. We use wiki text 2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data\n",
    "from importlib import reload\n",
    "#reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data' from '/home/manoj/Documents/github/language-modelling/data.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and batchify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "work_dir = os.getcwd()\n",
    "data_path = os.path.join(work_dir, 'data/wikitext-2-raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = data.Corpus(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    num_batches = data.size(0) // batch_size\n",
    "    data = data.narrow(0, 0, num_batches * batch_size)\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "valid_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.cuda.LongTensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2words(corpus, data):\n",
    "    return \" \".join([corpus.dictionary.idx2word[i] for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([104431, 20])\n",
      "torch.Size([21764, 10])\n",
      "torch.Size([24556, 10])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a basic RNN model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, num_tokens, emb_inp, num_hidden, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.encoder = nn.Embedding(num_tokens, emb_inp)\n",
    "        self.rnn = nn.RNN(emb_inp, num_hidden, num_layers, nonlinearity='tanh', dropout = 0.2)\n",
    "        self.decoder = nn.Linear(num_hidden, num_tokens)\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.encoder.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def init_hidden(self,bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.num_layers, bsz, self.num_hidden).zero_())\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "emb_size = 200\n",
    "num_hidden = 600\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(ntokens, emb_size, num_hidden, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (drop): Dropout(p=0.2)\n",
       "  (encoder): Embedding(84608, 200)\n",
       "  (rnn): RNN(200, 600, num_layers=2, dropout=0.2)\n",
       "  (decoder): Linear(in_features=600, out_features=84608)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combination of LogSoftmax and NLLLoss (Negative log likelihood loss) \n",
    "# Why this is used?\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bptt = 35 #seq_lentgh\n",
    "clip = 0.25 # Gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(source, i, evaluation=False):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def evaluate(valid_data):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for _,i in enumerate(range(0, valid_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(valid_data, i, evaluation=True)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss[0] / len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1\n",
    "log_interval = 200 #interval of batches to report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # We reset the hidden layer values each batch\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "    \n",
    "        #Clip gradients to handle exploding graadients\n",
    "        nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        #Updating the weights because of gradient clipping?\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "            \n",
    "        total_loss += loss.data\n",
    "        \n",
    "        #Logs\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss[0] / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_val_loss=None\n",
    "model_save_path = 'RNN_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | lr 1.00 | ms/batch 62.19 | loss 11.83 | ppl 137908.16\n",
      "| epoch   1 |   400/ 2983 batches | lr 1.00 | ms/batch 61.38 | loss 11.36 | ppl 85503.47\n",
      "| epoch   1 |   600/ 2983 batches | lr 1.00 | ms/batch 60.72 | loss 10.58 | ppl 39225.94\n",
      "| epoch   1 |   800/ 2983 batches | lr 1.00 | ms/batch 59.37 | loss 10.05 | ppl 23150.22\n",
      "| epoch   1 |  1000/ 2983 batches | lr 1.00 | ms/batch 58.14 | loss  9.80 | ppl 18038.83\n",
      "| epoch   1 |  1200/ 2983 batches | lr 1.00 | ms/batch 64.50 | loss  9.49 | ppl 13218.26\n",
      "| epoch   1 |  1400/ 2983 batches | lr 1.00 | ms/batch 63.73 | loss  9.44 | ppl 12599.34\n",
      "| epoch   1 |  1600/ 2983 batches | lr 1.00 | ms/batch 61.84 | loss  9.24 | ppl 10306.13\n",
      "| epoch   1 |  1800/ 2983 batches | lr 1.00 | ms/batch 59.88 | loss  9.12 | ppl  9143.81\n",
      "| epoch   1 |  2000/ 2983 batches | lr 1.00 | ms/batch 58.79 | loss  9.08 | ppl  8819.48\n",
      "| epoch   1 |  2200/ 2983 batches | lr 1.00 | ms/batch 56.71 | loss  8.98 | ppl  7929.33\n",
      "| epoch   1 |  2400/ 2983 batches | lr 1.00 | ms/batch 56.70 | loss  8.93 | ppl  7536.79\n",
      "| epoch   1 |  2600/ 2983 batches | lr 1.00 | ms/batch 56.84 | loss  8.86 | ppl  7052.75\n",
      "| epoch   1 |  2800/ 2983 batches | lr 1.00 | ms/batch 61.59 | loss  8.83 | ppl  6820.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 185.50s | valid loss  8.06 | valid ppl  3151.49\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manoj/anaconda3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |   200/ 2983 batches | lr 1.00 | ms/batch 61.41 | loss  8.20 | ppl  3647.40\n",
      "| epoch   2 |   400/ 2983 batches | lr 1.00 | ms/batch 59.00 | loss  8.04 | ppl  3115.58\n",
      "| epoch   2 |   600/ 2983 batches | lr 1.00 | ms/batch 58.87 | loss  7.97 | ppl  2884.45\n",
      "| epoch   2 |   800/ 2983 batches | lr 1.00 | ms/batch 58.56 | loss  7.93 | ppl  2784.90\n",
      "| epoch   2 |  1000/ 2983 batches | lr 1.00 | ms/batch 59.49 | loss  7.86 | ppl  2601.32\n",
      "| epoch   2 |  1200/ 2983 batches | lr 1.00 | ms/batch 59.65 | loss  7.89 | ppl  2665.56\n",
      "| epoch   2 |  1400/ 2983 batches | lr 1.00 | ms/batch 61.28 | loss  7.88 | ppl  2646.91\n",
      "| epoch   2 |  1600/ 2983 batches | lr 1.00 | ms/batch 58.85 | loss  7.87 | ppl  2629.33\n",
      "| epoch   2 |  1800/ 2983 batches | lr 1.00 | ms/batch 58.73 | loss  7.79 | ppl  2405.15\n",
      "| epoch   2 |  2000/ 2983 batches | lr 1.00 | ms/batch 58.55 | loss  7.79 | ppl  2422.83\n",
      "| epoch   2 |  2200/ 2983 batches | lr 1.00 | ms/batch 58.79 | loss  7.78 | ppl  2398.28\n",
      "| epoch   2 |  2400/ 2983 batches | lr 1.00 | ms/batch 58.91 | loss  7.76 | ppl  2355.70\n",
      "| epoch   2 |  2600/ 2983 batches | lr 1.00 | ms/batch 61.65 | loss  7.78 | ppl  2398.21\n",
      "| epoch   2 |  2800/ 2983 batches | lr 1.00 | ms/batch 63.69 | loss  7.75 | ppl  2323.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 185.08s | valid loss  7.74 | valid ppl  2309.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 1.00 | ms/batch 62.94 | loss  7.77 | ppl  2367.83\n",
      "| epoch   3 |   400/ 2983 batches | lr 1.00 | ms/batch 61.71 | loss  7.72 | ppl  2248.74\n",
      "| epoch   3 |   600/ 2983 batches | lr 1.00 | ms/batch 61.86 | loss  7.69 | ppl  2178.40\n",
      "| epoch   3 |   800/ 2983 batches | lr 1.00 | ms/batch 62.50 | loss  7.69 | ppl  2183.01\n",
      "| epoch   3 |  1000/ 2983 batches | lr 1.00 | ms/batch 62.40 | loss  7.63 | ppl  2049.48\n",
      "| epoch   3 |  1200/ 2983 batches | lr 1.00 | ms/batch 61.91 | loss  7.69 | ppl  2176.46\n",
      "| epoch   3 |  1400/ 2983 batches | lr 1.00 | ms/batch 63.02 | loss  7.71 | ppl  2222.27\n",
      "| epoch   3 |  1600/ 2983 batches | lr 1.00 | ms/batch 61.97 | loss  7.72 | ppl  2246.08\n",
      "| epoch   3 |  1800/ 2983 batches | lr 1.00 | ms/batch 61.82 | loss  7.64 | ppl  2074.54\n",
      "| epoch   3 |  2000/ 2983 batches | lr 1.00 | ms/batch 61.52 | loss  7.66 | ppl  2120.42\n",
      "| epoch   3 |  2200/ 2983 batches | lr 1.00 | ms/batch 62.11 | loss  7.66 | ppl  2117.26\n",
      "| epoch   3 |  2400/ 2983 batches | lr 1.00 | ms/batch 62.90 | loss  7.66 | ppl  2121.76\n",
      "| epoch   3 |  2600/ 2983 batches | lr 1.00 | ms/batch 62.02 | loss  7.68 | ppl  2160.34\n",
      "| epoch   3 |  2800/ 2983 batches | lr 1.00 | ms/batch 61.20 | loss  7.65 | ppl  2093.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 191.46s | valid loss  7.70 | valid ppl  2206.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 1.00 | ms/batch 62.04 | loss  7.70 | ppl  2197.82\n",
      "| epoch   4 |   400/ 2983 batches | lr 1.00 | ms/batch 61.31 | loss  7.65 | ppl  2109.65\n",
      "| epoch   4 |   600/ 2983 batches | lr 1.00 | ms/batch 61.25 | loss  7.63 | ppl  2052.43\n",
      "| epoch   4 |   800/ 2983 batches | lr 1.00 | ms/batch 61.31 | loss  7.64 | ppl  2073.13\n",
      "| epoch   4 |  1000/ 2983 batches | lr 1.00 | ms/batch 61.11 | loss  7.59 | ppl  1981.38\n",
      "| epoch   4 |  1200/ 2983 batches | lr 1.00 | ms/batch 61.51 | loss  7.64 | ppl  2084.03\n",
      "| epoch   4 |  1400/ 2983 batches | lr 1.00 | ms/batch 61.48 | loss  7.66 | ppl  2113.82\n",
      "| epoch   4 |  1600/ 2983 batches | lr 1.00 | ms/batch 61.78 | loss  7.67 | ppl  2144.16\n",
      "| epoch   4 |  1800/ 2983 batches | lr 1.00 | ms/batch 61.68 | loss  7.61 | ppl  2013.35\n",
      "| epoch   4 |  2000/ 2983 batches | lr 1.00 | ms/batch 61.38 | loss  7.62 | ppl  2047.05\n",
      "| epoch   4 |  2200/ 2983 batches | lr 1.00 | ms/batch 61.06 | loss  7.62 | ppl  2046.56\n",
      "| epoch   4 |  2400/ 2983 batches | lr 1.00 | ms/batch 61.23 | loss  7.62 | ppl  2043.11\n",
      "| epoch   4 |  2600/ 2983 batches | lr 1.00 | ms/batch 60.99 | loss  7.65 | ppl  2099.36\n",
      "| epoch   4 |  2800/ 2983 batches | lr 1.00 | ms/batch 61.65 | loss  7.62 | ppl  2029.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 189.16s | valid loss  7.70 | valid ppl  2213.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 0.25 | ms/batch 61.53 | loss  7.57 | ppl  1935.23\n",
      "| epoch   5 |   400/ 2983 batches | lr 0.25 | ms/batch 61.74 | loss  7.52 | ppl  1835.99\n",
      "| epoch   5 |   600/ 2983 batches | lr 0.25 | ms/batch 61.39 | loss  7.48 | ppl  1773.20\n",
      "| epoch   5 |   800/ 2983 batches | lr 0.25 | ms/batch 61.09 | loss  7.50 | ppl  1814.62\n",
      "| epoch   5 |  1000/ 2983 batches | lr 0.25 | ms/batch 61.26 | loss  7.44 | ppl  1703.80\n",
      "| epoch   5 |  1200/ 2983 batches | lr 0.25 | ms/batch 62.64 | loss  7.49 | ppl  1788.94\n",
      "| epoch   5 |  1400/ 2983 batches | lr 0.25 | ms/batch 61.01 | loss  7.51 | ppl  1832.94\n",
      "| epoch   5 |  1600/ 2983 batches | lr 0.25 | ms/batch 61.26 | loss  7.53 | ppl  1863.18\n",
      "| epoch   5 |  1800/ 2983 batches | lr 0.25 | ms/batch 61.16 | loss  7.45 | ppl  1722.97\n",
      "| epoch   5 |  2000/ 2983 batches | lr 0.25 | ms/batch 61.26 | loss  7.48 | ppl  1775.59\n",
      "| epoch   5 |  2200/ 2983 batches | lr 0.25 | ms/batch 61.09 | loss  7.48 | ppl  1768.69\n",
      "| epoch   5 |  2400/ 2983 batches | lr 0.25 | ms/batch 61.32 | loss  7.47 | ppl  1763.23\n",
      "| epoch   5 |  2600/ 2983 batches | lr 0.25 | ms/batch 61.58 | loss  7.50 | ppl  1803.99\n",
      "| epoch   5 |  2800/ 2983 batches | lr 0.25 | ms/batch 61.56 | loss  7.47 | ppl  1750.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 189.20s | valid loss  7.51 | valid ppl  1820.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2983 batches | lr 0.25 | ms/batch 62.10 | loss  7.55 | ppl  1891.91\n",
      "| epoch   6 |   400/ 2983 batches | lr 0.25 | ms/batch 61.66 | loss  7.50 | ppl  1804.92\n",
      "| epoch   6 |   600/ 2983 batches | lr 0.25 | ms/batch 61.00 | loss  7.47 | ppl  1747.17\n",
      "| epoch   6 |   800/ 2983 batches | lr 0.25 | ms/batch 60.77 | loss  7.49 | ppl  1792.80\n",
      "| epoch   6 |  1000/ 2983 batches | lr 0.25 | ms/batch 58.92 | loss  7.43 | ppl  1683.60\n",
      "| epoch   6 |  1200/ 2983 batches | lr 0.25 | ms/batch 56.65 | loss  7.48 | ppl  1768.80\n",
      "| epoch   6 |  1400/ 2983 batches | lr 0.25 | ms/batch 57.48 | loss  7.50 | ppl  1815.88\n",
      "| epoch   6 |  1600/ 2983 batches | lr 0.25 | ms/batch 56.34 | loss  7.52 | ppl  1841.73\n",
      "| epoch   6 |  1800/ 2983 batches | lr 0.25 | ms/batch 56.69 | loss  7.44 | ppl  1707.25\n",
      "| epoch   6 |  2000/ 2983 batches | lr 0.25 | ms/batch 57.72 | loss  7.48 | ppl  1770.15\n",
      "| epoch   6 |  2200/ 2983 batches | lr 0.25 | ms/batch 58.31 | loss  7.47 | ppl  1756.75\n",
      "| epoch   6 |  2400/ 2983 batches | lr 0.25 | ms/batch 57.76 | loss  7.47 | ppl  1756.60\n",
      "| epoch   6 |  2600/ 2983 batches | lr 0.25 | ms/batch 58.81 | loss  7.49 | ppl  1794.30\n",
      "| epoch   6 |  2800/ 2983 batches | lr 0.25 | ms/batch 57.76 | loss  7.47 | ppl  1748.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 180.47s | valid loss  7.51 | valid ppl  1822.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2983 batches | lr 0.06 | ms/batch 56.46 | loss  7.57 | ppl  1929.95\n",
      "| epoch   7 |   400/ 2983 batches | lr 0.06 | ms/batch 58.94 | loss  7.52 | ppl  1839.83\n",
      "| epoch   7 |   600/ 2983 batches | lr 0.06 | ms/batch 58.70 | loss  7.49 | ppl  1782.01\n",
      "| epoch   7 |   800/ 2983 batches | lr 0.06 | ms/batch 58.67 | loss  7.51 | ppl  1833.89\n",
      "| epoch   7 |  1000/ 2983 batches | lr 0.06 | ms/batch 57.01 | loss  7.44 | ppl  1706.34\n",
      "| epoch   7 |  1200/ 2983 batches | lr 0.06 | ms/batch 56.12 | loss  7.49 | ppl  1791.47\n",
      "| epoch   7 |  1400/ 2983 batches | lr 0.06 | ms/batch 56.55 | loss  7.51 | ppl  1830.76\n",
      "| epoch   7 |  1600/ 2983 batches | lr 0.06 | ms/batch 56.39 | loss  7.52 | ppl  1851.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |  1800/ 2983 batches | lr 0.06 | ms/batch 56.07 | loss  7.45 | ppl  1718.52\n",
      "| epoch   7 |  2000/ 2983 batches | lr 0.06 | ms/batch 56.11 | loss  7.48 | ppl  1774.77\n",
      "| epoch   7 |  2200/ 2983 batches | lr 0.06 | ms/batch 56.07 | loss  7.47 | ppl  1754.17\n",
      "| epoch   7 |  2400/ 2983 batches | lr 0.06 | ms/batch 57.31 | loss  7.47 | ppl  1749.32\n",
      "| epoch   7 |  2600/ 2983 batches | lr 0.06 | ms/batch 58.85 | loss  7.49 | ppl  1781.27\n",
      "| epoch   7 |  2800/ 2983 batches | lr 0.06 | ms/batch 58.98 | loss  7.45 | ppl  1727.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 178.89s | valid loss  7.47 | valid ppl  1762.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2983 batches | lr 0.06 | ms/batch 60.97 | loss  7.55 | ppl  1903.01\n",
      "| epoch   8 |   400/ 2983 batches | lr 0.06 | ms/batch 58.73 | loss  7.50 | ppl  1816.45\n",
      "| epoch   8 |   600/ 2983 batches | lr 0.06 | ms/batch 59.86 | loss  7.47 | ppl  1759.54\n",
      "| epoch   8 |   800/ 2983 batches | lr 0.06 | ms/batch 59.05 | loss  7.50 | ppl  1806.36\n",
      "| epoch   8 |  1000/ 2983 batches | lr 0.06 | ms/batch 58.86 | loss  7.43 | ppl  1692.81\n",
      "| epoch   8 |  1200/ 2983 batches | lr 0.06 | ms/batch 60.07 | loss  7.48 | ppl  1772.62\n",
      "| epoch   8 |  1400/ 2983 batches | lr 0.06 | ms/batch 58.99 | loss  7.51 | ppl  1820.06\n",
      "| epoch   8 |  1600/ 2983 batches | lr 0.06 | ms/batch 58.84 | loss  7.52 | ppl  1840.35\n",
      "| epoch   8 |  1800/ 2983 batches | lr 0.06 | ms/batch 60.82 | loss  7.45 | ppl  1712.49\n",
      "| epoch   8 |  2000/ 2983 batches | lr 0.06 | ms/batch 62.57 | loss  7.48 | ppl  1765.99\n",
      "| epoch   8 |  2200/ 2983 batches | lr 0.06 | ms/batch 63.58 | loss  7.47 | ppl  1750.46\n",
      "| epoch   8 |  2400/ 2983 batches | lr 0.06 | ms/batch 61.92 | loss  7.47 | ppl  1746.63\n",
      "| epoch   8 |  2600/ 2983 batches | lr 0.06 | ms/batch 62.05 | loss  7.49 | ppl  1790.10\n",
      "| epoch   8 |  2800/ 2983 batches | lr 0.06 | ms/batch 62.33 | loss  7.46 | ppl  1739.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 187.13s | valid loss  7.47 | valid ppl  1761.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2983 batches | lr 0.06 | ms/batch 62.11 | loss  7.55 | ppl  1891.29\n",
      "| epoch   9 |   400/ 2983 batches | lr 0.06 | ms/batch 62.19 | loss  7.50 | ppl  1805.48\n",
      "| epoch   9 |   600/ 2983 batches | lr 0.06 | ms/batch 62.39 | loss  7.47 | ppl  1751.90\n",
      "| epoch   9 |   800/ 2983 batches | lr 0.06 | ms/batch 61.74 | loss  7.49 | ppl  1798.92\n",
      "| epoch   9 |  1000/ 2983 batches | lr 0.06 | ms/batch 62.22 | loss  7.43 | ppl  1682.93\n",
      "| epoch   9 |  1200/ 2983 batches | lr 0.06 | ms/batch 62.36 | loss  7.48 | ppl  1768.17\n",
      "| epoch   9 |  1400/ 2983 batches | lr 0.06 | ms/batch 62.25 | loss  7.51 | ppl  1818.36\n",
      "| epoch   9 |  1600/ 2983 batches | lr 0.06 | ms/batch 61.97 | loss  7.51 | ppl  1829.02\n",
      "| epoch   9 |  1800/ 2983 batches | lr 0.06 | ms/batch 63.93 | loss  7.44 | ppl  1708.44\n",
      "| epoch   9 |  2000/ 2983 batches | lr 0.06 | ms/batch 63.09 | loss  7.48 | ppl  1769.66\n",
      "| epoch   9 |  2200/ 2983 batches | lr 0.06 | ms/batch 62.23 | loss  7.47 | ppl  1753.09\n",
      "| epoch   9 |  2400/ 2983 batches | lr 0.06 | ms/batch 61.95 | loss  7.47 | ppl  1746.45\n",
      "| epoch   9 |  2600/ 2983 batches | lr 0.06 | ms/batch 63.57 | loss  7.49 | ppl  1788.89\n",
      "| epoch   9 |  2800/ 2983 batches | lr 0.06 | ms/batch 63.89 | loss  7.46 | ppl  1739.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 193.18s | valid loss  7.47 | valid ppl  1762.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2983 batches | lr 0.02 | ms/batch 65.12 | loss  7.56 | ppl  1913.53\n",
      "| epoch  10 |   400/ 2983 batches | lr 0.02 | ms/batch 62.92 | loss  7.51 | ppl  1827.73\n",
      "| epoch  10 |   600/ 2983 batches | lr 0.02 | ms/batch 62.73 | loss  7.48 | ppl  1768.44\n",
      "| epoch  10 |   800/ 2983 batches | lr 0.02 | ms/batch 62.52 | loss  7.50 | ppl  1816.54\n",
      "| epoch  10 |  1000/ 2983 batches | lr 0.02 | ms/batch 63.05 | loss  7.44 | ppl  1698.64\n",
      "| epoch  10 |  1200/ 2983 batches | lr 0.02 | ms/batch 64.59 | loss  7.48 | ppl  1776.75\n",
      "| epoch  10 |  1400/ 2983 batches | lr 0.02 | ms/batch 62.11 | loss  7.51 | ppl  1825.72\n",
      "| epoch  10 |  1600/ 2983 batches | lr 0.02 | ms/batch 64.33 | loss  7.52 | ppl  1835.50\n",
      "| epoch  10 |  1800/ 2983 batches | lr 0.02 | ms/batch 63.97 | loss  7.44 | ppl  1709.68\n",
      "| epoch  10 |  2000/ 2983 batches | lr 0.02 | ms/batch 63.96 | loss  7.48 | ppl  1770.95\n",
      "| epoch  10 |  2200/ 2983 batches | lr 0.02 | ms/batch 62.94 | loss  7.47 | ppl  1747.37\n",
      "| epoch  10 |  2400/ 2983 batches | lr 0.02 | ms/batch 63.62 | loss  7.46 | ppl  1737.60\n",
      "| epoch  10 |  2600/ 2983 batches | lr 0.02 | ms/batch 63.99 | loss  7.48 | ppl  1779.20\n",
      "| epoch  10 |  2800/ 2983 batches | lr 0.02 | ms/batch 63.96 | loss  7.45 | ppl  1724.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 195.76s | valid loss  7.47 | valid ppl  1749.86\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# At any point you can stop kernel to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(valid_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(model_save_path, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outf = \"generated.txt\"\n",
    "no_of_words = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate():\n",
    "    input = Variable(torch.rand(1,1).mul(ntokens).long(), volatile=True)\n",
    "    input.data = input.data.cuda()\n",
    "    hidden = model.init_hidden(1)\n",
    "    with open(outf, 'w') as of:\n",
    "        for i in range(no_of_words):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().data.div(1.0).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.data.fill_(word_idx)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "            of.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
